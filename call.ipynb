{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bdce119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47882544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RAG Chat System...\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models loaded successfully!\n",
      "üìö Created new document collection\n",
      "\n",
      "üéâ RAG Chat System Ready!\n",
      "\n",
      "Example usage:\n",
      "\n",
      "1. Upload a PDF:\n",
      "   upload_pdf(\"path/to/your/document.pdf\")\n",
      "\n",
      "2. Start chatting:\n",
      "   chat_with_pdf(\"What is this document about?\")\n",
      "   chat_with_pdf(\"Summarize the key points\")\n",
      "   chat_with_pdf(\"What are the main conclusions?\")\n",
      "\n",
      "3. Utility functions:\n",
      "   show_chat_history()      # View all conversations\n",
      "   clear_chat_history()     # Clear chat history\n",
      "   show_database_info()     # Check database status\n",
      "\n",
      "üìù Example:\n",
      "   upload_pdf(\"research_paper.pdf\")\n",
      "   chat_with_pdf(\"What is the main research question?\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RAGChatSystem:\n",
    "    def __init__(self):\n",
    "        self.chat_history = []\n",
    "        self.setup_models()\n",
    "        self.setup_chromadb()\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize the embedding and generation models\"\"\"\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        # Use sentence-transformers for better embeddings\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Use GPT-2 for text generation\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.generator = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        \n",
    "        # Add padding token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(\"‚úÖ Models loaded successfully!\")\n",
    "        \n",
    "    def setup_chromadb(self):\n",
    "        \"\"\"Initialize ChromaDB client\"\"\"\n",
    "        # Create a persistent ChromaDB client\n",
    "        self.chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        \n",
    "        # Create or get collection\n",
    "        try:\n",
    "            self.collection = self.chroma_client.get_collection(\"pdf_documents\")\n",
    "            print(\"üìö Connected to existing document collection\")\n",
    "        except:\n",
    "            self.collection = self.chroma_client.create_collection(\"pdf_documents\")\n",
    "            print(\"üìö Created new document collection\")\n",
    "            \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting text from PDF: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        # Clean text\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "        return chunks\n",
    "    \n",
    "    def add_document_to_db(self, text: str, filename: str):\n",
    "        \"\"\"Process and add document to ChromaDB\"\"\"\n",
    "        print(f\"üìÑ Processing document: {filename}\")\n",
    "        \n",
    "        # Chunk the text\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"‚ùå No text chunks generated from the PDF\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"üìù Generated {len(chunks)} text chunks\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(\"üîç Creating embeddings...\")\n",
    "        embeddings = self.embedding_model.encode(chunks).tolist()\n",
    "        \n",
    "        # Create unique IDs for chunks\n",
    "        ids = [f\"{filename}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        \n",
    "        # Add to ChromaDB\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                embeddings=embeddings,\n",
    "                documents=chunks,\n",
    "                ids=ids,\n",
    "                metadatas=[{\"filename\": filename, \"chunk_id\": i} for i in range(len(chunks))]\n",
    "            )\n",
    "            print(f\"‚úÖ Successfully added {len(chunks)} chunks to database\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error adding document to database: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, n_results: int = 3) -> List[str]:\n",
    "        \"\"\"Retrieve relevant text chunks based on query\"\"\"\n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_model.encode([query]).tolist()\n",
    "            \n",
    "            # Search in ChromaDB\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_embedding,\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            return results['documents'][0] if results['documents'] else []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving documents: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def generate_response(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"Generate response using GPT-2 with retrieved context\"\"\"\n",
    "        # Combine context chunks\n",
    "        context = \"\\n\".join(context_chunks)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Based on the following context, please answer the question:\n",
    "\n",
    "Context: {context[:800]}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        try:\n",
    "            inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.generator.generate(\n",
    "                    inputs,\n",
    "                    max_length=inputs.shape[1] + 100,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    attention_mask=torch.ones_like(inputs)\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the generated part (after the prompt)\n",
    "            answer_start = response.find(\"Answer:\") + len(\"Answer:\")\n",
    "            response = response[answer_start:].strip()\n",
    "            \n",
    "            return response if response else \"I couldn't generate a response based on the provided context.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def chat(self, query: str) -> str:\n",
    "        \"\"\"Main chat function that combines retrieval and generation\"\"\"\n",
    "        print(f\"üîç Searching for: {query}\")\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            response = \"I couldn't find relevant information in the uploaded documents to answer your question.\"\n",
    "        else:\n",
    "            print(f\"üìö Found {len(relevant_chunks)} relevant chunks\")\n",
    "            # Generate response\n",
    "            response = self.generate_response(query, relevant_chunks)\n",
    "        \n",
    "        # Add to chat history\n",
    "        self.chat_history.append({\"user\": query, \"assistant\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_database_info(self):\n",
    "        \"\"\"Get information about the current database\"\"\"\n",
    "        try:\n",
    "            count = self.collection.count()\n",
    "            return f\"Database contains {count} document chunks\"\n",
    "        except:\n",
    "            return \"Database is empty\"\n",
    "\n",
    "# Cell 4: Initialize the system\n",
    "print(\"üöÄ Initializing RAG Chat System...\")\n",
    "rag_system = RAGChatSystem()\n",
    "\n",
    "# Cell 5: Upload and process PDF function\n",
    "def upload_pdf(pdf_path: str):\n",
    "    \"\"\"Upload and process a PDF file\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"‚ùå File not found: {pdf_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üìÅ Loading PDF: {pdf_path}\")\n",
    "    \n",
    "    # Extract text\n",
    "    text = rag_system.extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    if text:\n",
    "        print(f\"üìÑ Extracted {len(text)} characters\")\n",
    "        # Add to database\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        success = rag_system.add_document_to_db(text, filename)\n",
    "        return success\n",
    "    else:\n",
    "        print(\"‚ùå Could not extract text from PDF\")\n",
    "        return False\n",
    "\n",
    "# Cell 6: Chat function\n",
    "def chat_with_pdf(query: str):\n",
    "    \"\"\"Chat with the uploaded PDF\"\"\"\n",
    "    if not query.strip():\n",
    "        print(\"‚ö†Ô∏è Please enter a question!\")\n",
    "        return\n",
    "    \n",
    "    response = rag_system.chat(query)\n",
    "    \n",
    "    # Display the conversation\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üë§ You: {query}\")\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Cell 7: Utility functions\n",
    "def show_chat_history():\n",
    "    \"\"\"Display the full chat history\"\"\"\n",
    "    print(\"üí¨ CHAT HISTORY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not rag_system.chat_history:\n",
    "        print(\"No conversations yet. Start chatting!\")\n",
    "        return\n",
    "    \n",
    "    for i, chat in enumerate(rag_system.chat_history, 1):\n",
    "        print(f\"[{i}] üë§ You: {chat['user']}\")\n",
    "        print(f\"[{i}] ü§ñ Assistant: {chat['assistant']}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "def clear_chat_history():\n",
    "    \"\"\"Clear the chat history\"\"\"\n",
    "    rag_system.chat_history = []\n",
    "    print(\"üóëÔ∏è Chat history cleared!\")\n",
    "\n",
    "def show_database_info():\n",
    "    \"\"\"Show database information\"\"\"\n",
    "    info = rag_system.get_database_info()\n",
    "    print(f\"üìä {info}\")\n",
    "\n",
    "# Cell 8: Example usage\n",
    "print(\"\"\"\n",
    "üéâ RAG Chat System Ready!\n",
    "\n",
    "Example usage:\n",
    "\n",
    "1. Upload a PDF:\n",
    "   upload_pdf(\"path/to/your/document.pdf\")\n",
    "\n",
    "2. Start chatting:\n",
    "   chat_with_pdf(\"What is this document about?\")\n",
    "   chat_with_pdf(\"Summarize the key points\")\n",
    "   chat_with_pdf(\"What are the main conclusions?\")\n",
    "\n",
    "3. Utility functions:\n",
    "   show_chat_history()      # View all conversations\n",
    "   clear_chat_history()     # Clear chat history\n",
    "   show_database_info()     # Check database status\n",
    "\n",
    "üìù Example:\n",
    "   upload_pdf(\"research_paper.pdf\")\n",
    "   chat_with_pdf(\"What is the main research question?\")\n",
    "\"\"\")\n",
    "\n",
    "# Cell 9: Interactive widgets (optional)\n",
    "def create_interactive_interface():\n",
    "    \"\"\"Create an interactive widget interface\"\"\"\n",
    "    \n",
    "    # File upload widget\n",
    "    file_upload = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter PDF file path...',\n",
    "        description='PDF Path:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    upload_button = widgets.Button(\n",
    "        description='Upload PDF',\n",
    "        disabled=False,\n",
    "        button_style='success',\n",
    "        tooltip='Process the PDF file'\n",
    "    )\n",
    "    \n",
    "    # Chat widgets\n",
    "    query_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Ask a question about your PDF...',\n",
    "        description='Question:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    chat_button = widgets.Button(\n",
    "        description='Send',\n",
    "        disabled=False,\n",
    "        button_style='primary',\n",
    "        tooltip='Send your question'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_upload_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            if file_upload.value:\n",
    "                upload_pdf(file_upload.value)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Please enter a PDF file path\")\n",
    "    \n",
    "    def on_chat_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            if query_input.value:\n",
    "                chat_with_pdf(query_input.value)\n",
    "                query_input.value = \"\"  # Clear input after sending\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Please enter a question\")\n",
    "    \n",
    "    upload_button.on_click(on_upload_click)\n",
    "    chat_button.on_click(on_chat_click)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üìö RAG Chat Interface</h3>\"),\n",
    "        widgets.HBox([file_upload, upload_button]),\n",
    "        widgets.HBox([query_input, chat_button]),\n",
    "        output_area\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b6005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604d05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218c701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3b433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94881184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd0465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0b590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e763c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d20ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
